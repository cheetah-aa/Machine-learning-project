Hyperparameter Tuning
To improve model performance and reduce overfitting, we applied hyperparameter tuning using GridSearchCV:
XGBoost: Tuned learning_rate, max_depth, and n_estimators via random search with cross-validation.
Selected parameter combinations yielded improvements in both accuracy and generalization.
XGBoost Tuning Details:
import xgboost as xgb
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt

# Define the initial XGBoost model
xgboost_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Set up hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0]
}

# Create GridSearchCV object
grid_search = GridSearchCV(
    estimator=xgboost_model,
    param_grid=param_grid,
    scoring='accuracy',
    cv=3,
    verbose=1,
    n_jobs=-1  # Use all available cores
)

# Perform grid search
grid_search.fit(X_train, y_train_binary)

# Extract best parameters and score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Cross-Validated Accuracy:", round(best_score * 100, 2), "%")

# Train best model on full training data
best_xgboost_model = xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss')
best_xgboost_model.fit(X_train, y_train_binary)

# Feature importance visualization
plt.figure(figsize=(10, 6))
plt.barh(X.columns, best_xgboost_model.feature_importances_, color='teal')
plt.xlabel('Feature Importance Score')
plt.title('XGBoost Feature Importance')
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()
